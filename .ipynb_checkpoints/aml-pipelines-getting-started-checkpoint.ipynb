{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Pipelines: Getting Started\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "A common scenario when using machine learning components is to have a data workflow that includes the following steps:\n",
    "\n",
    "- Preparing/preprocessing a given dataset for training, followed by\n",
    "- Training a machine learning model on this data, and then\n",
    "- Deploying this trained model in a separate environment, and finally\n",
    "- Running a batch scoring task on another data set, using the trained model.\n",
    "\n",
    "Azure's Machine Learning pipelines give you a way to combine multiple steps like these into one configurable workflow, so that multiple agents/users can share and/or reuse this workflow. Machine learning pipelines thus provide a consistent, reproducible mechanism for building, evaluating, deploying, and running ML systems.\n",
    "\n",
    "To get more information about Azure machine learning pipelines, please read our [Azure Machine Learning Pipelines](https://aka.ms/pl-concept) overview, or the [readme article](https://aka.ms/pl-readme).\n",
    "\n",
    "In this notebook, we provide a gentle introduction to Azure machine learning pipelines. We build a pipeline that runs jobs unattended on different compute clusters; in this notebook, you'll see how to use the basic Azure ML SDK APIs for constructing this pipeline.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Azure Machine Learning Basics\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the [configuration notebook](https://aka.ms/pl-config) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Machine Learning Imports\n",
    "\n",
    "In this first code cell, we import key Azure Machine Learning modules that we will use below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.72\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline-specific SDK imports\n",
    "\n",
    "Here, we import key pipeline modules, whose use will be illustrated in the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline SDK-specific imports completed\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "print(\"Pipeline SDK-specific imports completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workspace\n",
    "\n",
    "Initialize a [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class%29) object from persisted configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "create workspace"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mtcs-dev-aml\n",
      "mtcs-aml-rg\n",
      "westus2\n",
      "256c7222-4083-4ba7-8714-baa0df54bfe6\n",
      "Blobstore's name: workspaceblobstore\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "\n",
    "# Default datastore\n",
    "def_blob_store = ws.get_default_datastore() \n",
    "# The following call GETS the Azure Blob Store associated with your workspace.\n",
    "# Note that workspaceblobstore is **the name of this store and CANNOT BE CHANGED and must be used as is** \n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required data and script files for the the tutorial\n",
    "Sample files required to finish this tutorial are already copied to the corresponding source_directory locations. Even though the .py provided in the samples does not have much \"ML work\" as a data scientist, you will work on this extensively as part of your work. To complete this tutorial, the contents of these files are not very important. The one-line files are for demostration purpose only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datastore concepts\n",
    "A [Datastore](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py) is a place where data can be stored that is then made accessible to a compute either by means of mounting or copying the data to the compute target. \n",
    "\n",
    "A Datastore can either be backed by an Azure File Storage (default) or by an Azure Blob Storage.\n",
    "\n",
    "In this next step, we will upload the training and test set into the workspace's default storage (File storage), and another piece of data to Azure Blob Storage. When to use [Azure Blobs](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction), [Azure Files](https://docs.microsoft.com/en-us/azure/storage/files/storage-files-introduction), or [Azure Disks](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/managed-disks-overview) is [detailed here](https://docs.microsoft.com/en-us/azure/storage/common/storage-decide-blobs-files-disks).\n",
    "\n",
    "**Please take good note of the concept of the datastore.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to default datastore\n",
    "Default datastore on workspace is the Azure  File storage. The workspace has a Blob storage associated with it as well. Let's upload a file to each of these storages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import PipelineData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./amlpipedata.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./amlpipedata.csv\n",
    "idx,num,msg\n",
    "1,1,init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading ./amlpipedata.csv\n",
      "Uploaded ./amlpipedata.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    }
   ],
   "source": [
    "# get_default_datastore() gets the default Azure Blob Store associated with your workspace.\n",
    "# Here we are reusing the def_blob_store object we obtained earlier\n",
    "\n",
    "filename = \"amlpipedata.csv\"\n",
    "file_path = os.path.join(os.getcwd(),filename)\n",
    "# print(file_path)\n",
    "\n",
    "def_blob_store.upload_files([\"./amlpipedata.csv\"], target_path=\"sampledata\", overwrite=True)\n",
    "# print(\"Upload call completed\")\n",
    "\n",
    "blob_input_data = DataReference(\n",
    "    datastore=def_blob_store,\n",
    "    data_reference_name=\"test_data\",\n",
    "    path_on_datastore=\"sampledata/amlpipedata.csv\")\n",
    "\n",
    "# print(\"DataReference object created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_blob_store = Datastore(ws, \"output\")\n",
    "\n",
    "blob_output_data = DataReference(\n",
    "    datastore=out_blob_store,\n",
    "    data_reference_name=\"interm_data\",\n",
    "    path_on_datastore=\"interm\"\n",
    ")\n",
    "\n",
    "interm_pipelineData = PipelineData(\n",
    "    name=\"interm_pipelinedata\", \n",
    "    datastore=out_blob_store, \n",
    "    output_name=\"amlpipedata\", \n",
    "    output_overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Targets\n",
    "A compute target specifies where to execute your program such as a remote Docker on a VM, or a cluster. A compute target needs to be addressable and accessible by you.\n",
    "\n",
    "**You need at least one compute target to send your payload to. We are planning to use Azure Machine Learning Compute exclusively for this tutorial for all steps. However in some cases you may require multiple compute targets as some steps may run in one compute target like Azure Machine Learning Compute, and some other steps in the same pipeline could run in a different compute target.**\n",
    "\n",
    "*The example belows show creating/retrieving/attaching to an Azure Machine Learning Compute instance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Compute Targets on the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cts = ws.compute_targets\n",
    "# for ct in cts:\n",
    "#     print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve or create a Azure Machine Learning compute\n",
    "Azure Machine Learning Compute is a service for provisioning and managing clusters of Azure virtual machines for running machine learning workloads. Let's create a new Azure Machine Learning Compute in the current workspace, if it doesn't already exist. We will then run the training script on this compute target.\n",
    "\n",
    "If we could not find the compute with the given name in the previous cell, then we will create a new compute here. We will create an Azure Machine Learning Compute containing **STANDARD_D2_V2 CPU VMs**. This process is broken down into the following steps:\n",
    "\n",
    "1. Create the configuration\n",
    "2. Create the Azure Machine Learning compute\n",
    "\n",
    "**This process will take about 3 minutes and is providing only sparse output in the process. Please make sure to wait until the call returns before moving to the next cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found existing compute target.\n",
      "Azure Machine Learning Compute attached\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "aml_compute_target = \"aml-cpu-2\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new compute target\")\n",
    "    \n",
    "#     provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "#                                                                 min_nodes = 1, \n",
    "#                                                                 max_nodes = 4)    \n",
    "#     aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "#     aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "print(\"Azure Machine Learning Compute attached\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmlCompute(workspace=Workspace.create(name='mtcs-dev-aml', subscription_id='256c7222-4083-4ba7-8714-baa0df54bfe6', resource_group='mtcs-aml-rg'), name=aml-cpu-2, id=/subscriptions/256c7222-4083-4ba7-8714-baa0df54bfe6/resourceGroups/mtcs-aml-rg/providers/Microsoft.MachineLearningServices/workspaces/mtcs-dev-aml/computes/aml-cpu-2, type=AmlCompute, provisioning_state=Succeeded, location=westus2, tags=None)\n"
     ]
    }
   ],
   "source": [
    "# For a more detailed view of current Azure Machine Learning Compute status, use get_status()\n",
    "# example: un-comment the following line.\n",
    "print(aml_compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wait for this call to finish before proceeding (you will see the asterisk turning to a number).**\n",
    "\n",
    "Now that you have created the compute target, let's see what the workspace's compute_targets() function returns. You should now see one entry named 'amlcompute' of type AmlCompute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have completed learning the basics of Azure Machine Learning (AML), let's go ahead and start understanding the Pipeline concepts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Step in a Pipeline\n",
    "A Step is a unit of execution. Step typically needs a target of execution (compute target), a script to execute, and may require script arguments and inputs, and can produce outputs. The step also could take a number of other parameters. Azure Machine Learning Pipelines provides the following built-in Steps:\n",
    "\n",
    "- [**PythonScriptStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep?view=azure-ml-py): Adds a step to run a Python script in a Pipeline.\n",
    "- [**AdlaStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.adla_step.adlastep?view=azure-ml-py): Adds a step to run U-SQL script using Azure Data Lake Analytics.\n",
    "- [**DataTransferStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.data_transfer_step.datatransferstep?view=azure-ml-py): Transfers data between Azure Blob and Data Lake accounts.\n",
    "- [**DatabricksStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.databricks_step.databricksstep?view=azure-ml-py): Adds a DataBricks notebook as a step in a Pipeline.\n",
    "- [**HyperDriveStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.hyper_drive_step.hyperdrivestep?view=azure-ml-py): Creates a Hyper Drive step for Hyper Parameter Tuning in a Pipeline.\n",
    "- [**AzureBatchStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.azurebatch_step.azurebatchstep?view=azure-ml-py): Creates a step for submitting jobs to Azure Batch\n",
    "- [**EstimatorStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.estimator_step.estimatorstep?view=azure-ml-py): Adds a step to run Estimator in a Pipeline.\n",
    "- [**MpiStep**](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.mpi_step.mpistep?view=azure-ml-py): Adds a step to run a MPI job in a Pipeline.\n",
    "- [**AutoMLStep**](https://docs.microsoft.com/en-us/python/api/azureml-train-automl/azureml.train.automl.automlstep?view=azure-ml-py): Creates a AutoML step in a Pipeline.\n",
    "\n",
    "The following code will create a PythonScriptStep to be executed in the Azure Machine Learning Compute we created above using train.py, one of the files already made available in the `source_directory`.\n",
    "\n",
    "A **PythonScriptStep** is a basic, built-in step to run a Python Script on a compute target. It takes a script name and optionally other parameters like arguments for the script, compute target, inputs and outputs. If no compute target is specified, default compute target for the workspace is used. You can also use a [**RunConfiguration**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfiguration?view=azure-ml-py) to specify requirements for the PythonScriptStep, such as conda dependencies and docker image.\n",
    "> The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses default values for PythonScriptStep construct.\n",
    "\n",
    "source_directory = os.path.join(os.getcwd(),\"train\")\n",
    "\n",
    "PythonStep = PythonScriptStep(name=\"train_step\",\n",
    "                         script_name=\"train.py\", \n",
    "                         compute_target=aml_compute, \n",
    "                         arguments=[\"--input_data\", blob_input_data, \"--output_train\", interm_pipelineData],\n",
    "                         inputs=[blob_input_data],\n",
    "                         outputs=[interm_pipelineData],\n",
    "                         source_directory=source_directory,\n",
    "                         allow_reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In the above call to PythonScriptStep(), the flag *allow_reuse* determines whether the step should reuse previous results when run with the same settings/inputs. This flag's default value is *True*; the default is set to *True* because, when inputs and parameters have not changed, we typically do not want to re-run a given pipeline step. \n",
    "\n",
    "If *allow_reuse* is set to *False*, a new run will always be generated for this step during pipeline execution. The *allow_reuse* flag can come in handy in situations where you do *not* want to re-run a pipeline step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config input and output refrence for ADB Step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference the data uploaded to blob storage using a PipelineParameter and a DataPath\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "from azureml.data.datapath import DataPath, DataPathComputeBinding\n",
    "\n",
    "datapath = DataPath(datastore=out_blob_store, path_on_datastore='interm')\n",
    "datapath_param = PipelineParameter(name=\"intrem_data\", default_value=datapath)\n",
    "interm_pipelineData = (datapath_param, DataPathComputeBinding(mode='mount'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataPath.create_data_reference of <azureml.data.datapath.DataPath object at 0x7f88f830ecc0>>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath.create_data_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.pipeline.core import PipelineParameter\n",
    "\n",
    "# pipeline_param = PipelineParameter(name=\"my_pipeline_param\", default_value=\"pipeline_param1\")\n",
    "\n",
    "# Here is input for ADB step\n",
    "# processed_data1 = PipelineData(\"processed_data1\",datastore=def_blob_store)\n",
    "adb_output = PipelineData(\"adb_output\", datastore=out_blob_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interm_pipelineData.as_input(\"interm_input_for_ADB\")\n",
    "# interm_pipelineData.as_mount(input_name=\"amlpipe_adb_step\")\n",
    "# interm_pipelineData.as_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.data import dbfs_datastore\n",
    "# Datastore.register_dbfs(workspace=ws, datastore_name=\"aml_adb_dbfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks as Compute Target \n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-create-your-first-pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute target adb-dataprep already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatabricksCompute(workspace=Workspace.create(name='mtcs-dev-aml', subscription_id='256c7222-4083-4ba7-8714-baa0df54bfe6', resource_group='mtcs-aml-rg'), name=adb-dataprep, id=/subscriptions/256c7222-4083-4ba7-8714-baa0df54bfe6/resourceGroups/mtcs-aml-rg/providers/Microsoft.MachineLearningServices/workspaces/mtcs-dev-aml/computes/adb-dataprep, type=Databricks, provisioning_state=Succeeded, location=westus2, tags=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.compute import DatabricksCompute\n",
    "\n",
    "# Replace with your account info before running.\n",
    " \n",
    "db_compute_name=os.getenv(\"DATABRICKS_COMPUTE_NAME\", \"adb-dataprep\") # Databricks compute name\n",
    "db_resource_group=os.getenv(\"DATABRICKS_RESOURCE_GROUP\", \"mtcs-dev-databrick\") # Databricks resource group\n",
    "db_workspace_name=os.getenv(\"DATABRICKS_WORKSPACE_NAME\", \"mtcs-dev\") # Databricks workspace name\n",
    "db_access_token=os.getenv(\"DATABRICKS_ACCESS_TOKEN\", \"dapi78fb8c0624e5885da48fe1bd63fa1a5b\") # Databricks access token\n",
    "\n",
    "try:\n",
    "    databricks_compute = DatabricksCompute(workspace=ws, name=db_compute_name)\n",
    "    print('Compute target {} already exists'.format(db_compute_name))\n",
    "except ComputeTargetException:\n",
    "    print('Compute not found, will use below parameters to attach new one')\n",
    "    print('db_compute_name {}'.format(db_compute_name))\n",
    "    print('db_resource_group {}'.format(db_resource_group))\n",
    "    print('db_workspace_name {}'.format(db_workspace_name))\n",
    "    print('db_access_token {}'.format(db_access_token))\n",
    " \n",
    "    config = DatabricksCompute.attach_configuration(\n",
    "        resource_group = db_resource_group,\n",
    "        workspace_name = db_workspace_name,\n",
    "        access_token= db_access_token)\n",
    "    databricks_compute=ComputeTarget.attach(ws, db_compute_name, config)\n",
    "    databricks_compute.wait_for_completion(True)\n",
    "\n",
    "databricks_compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.databricks import PyPiLibrary\n",
    "from azureml.pipeline.steps import DatabricksStep\n",
    "\n",
    "notebook_path=os.getenv(\"DATABRICKS_NOTEBOOK_PATH\", \"/aml-pipelines/aml-pipeline-adb-step\") # Databricks notebook path\n",
    "amlLib = PyPiLibrary(package=\"azureml-sdk[automl_databricks]\")\n",
    "# amlLib = PyPiLibrary(package=\"azureml-sdk[databricks]\")\n",
    "\n",
    "ADBStep = DatabricksStep(\n",
    "    name=\"AML_Pipeline_ADB_Step\",\n",
    "#     inputs=[interm_pipelineData],\n",
    "#     outputs=[adb_output],\n",
    "    num_workers=1,\n",
    "    notebook_path=notebook_path,\n",
    "    notebook_params={'interm_pipelineData': interm_pipelineData},\n",
    "    run_name='AML_Pipeline_Run',\n",
    "    compute_target=databricks_compute,\n",
    "    existing_cluster_id = \"0727-170559-robin360\",\n",
    "#     spark_version= \"5.5.x-cpu-ml-scala2.11\",\n",
    "    pypi_libraries=[amlLib],\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline\n",
    "Once we have the steps (or steps collection), we can build the [pipeline](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py). By deafult, all these steps will run in **parallel** once we submit the pipeline for run.\n",
    "\n",
    "A pipeline is created with a list of steps and a workspace. Submit a pipeline using [submit](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment(class)?view=azure-ml-py#submit-config--tags-none----kwargs-). When submit is called, a [PipelineRun](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinerun?view=azure-ml-py) is created which in turn creates [StepRun](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.steprun?view=azure-ml-py) objects for each step in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n"
     ]
    }
   ],
   "source": [
    "ADBStep.run_after(PythonStep)\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[ADBStep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline\n",
    "[Submitting](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py#submit) the pipeline involves creating an [Experiment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment?view=azure-ml-py) object and providing the built pipeline for submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step AML_Pipeline_ADB_Step [35abaa6d][11578713-702e-4b8b-9c73-025c12b92070], (This step will run and generate new outputs)Created step train_step [62f04315][4cbe5771-707c-4d2c-937e-5890f9b72191], (This step is eligible to reuse a previous run's output)\n",
      "\n",
      "Created data reference output_61c636ef for StepId [dd440a74][f3a67e8a-331a-49d9-bebd-f37973c890b8], (Consumers of this data will generate new runs.)\n",
      "Using data reference test_data for StepId [0b04a04e][53e84b3f-2fc3-4636-9e3f-6fbdad53b56d], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted PipelineRun 2275167d-1ae7-4dd4-afbc-2dad13ac404f\n",
      "Link to Azure Machine Learning studio: https://ml.azure.com/experiments/Hello_World2/runs/2275167d-1ae7-4dd4-afbc-2dad13ac404f?wsid=/subscriptions/256c7222-4083-4ba7-8714-baa0df54bfe6/resourcegroups/mtcs-aml-rg/workspaces/mtcs-dev-aml\n",
      "Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "pipeline_run = Experiment(ws, 'Hello_World2').submit(pipeline)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the pipeline run\n",
    "\n",
    "#### Use RunDetails Widget\n",
    "We are going to use the RunDetails widget to examine the run of the pipeline. You can click each row below to get more details on the step runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78f395417fd4434a92e318a4b252e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/Hello_World2/runs/2275167d-1ae7-4dd4-afbc-2dad13ac404f?wsid=/subscriptions/256c7222-4083-4ba7-8714-baa0df54bfe6/resourcegroups/mtcs-aml-rg/workspaces/mtcs-dev-aml\", \"run_id\": \"2275167d-1ae7-4dd4-afbc-2dad13ac404f\", \"run_properties\": {\"run_id\": \"2275167d-1ae7-4dd4-afbc-2dad13ac404f\", \"created_utc\": \"2019-11-08T06:42:17.867194Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": null, \"runType\": \"HTTP\", \"azureml.parameters\": \"{}\"}, \"tags\": {\"azureml.pipelineComponent\": \"pipelinerun\"}, \"end_time_utc\": \"2019-11-08T06:42:27.259025Z\", \"status\": \"Failed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://mtcsdevaml3870043637.blob.core.windows.net/azureml/ExperimentRun/dcid.2275167d-1ae7-4dd4-afbc-2dad13ac404f/logs/azureml/executionlogs.txt?sv=2018-11-09&sr=b&sig=twjysGkPTyopRvV%2Bu5jQ0rXqziWq335cZRSyWUnj8tE%3D&st=2019-11-08T06%3A32%3A52Z&se=2019-11-08T14%3A42%3A52Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://mtcsdevaml3870043637.blob.core.windows.net/azureml/ExperimentRun/dcid.2275167d-1ae7-4dd4-afbc-2dad13ac404f/logs/azureml/stderrlogs.txt?sv=2018-11-09&sr=b&sig=F95L8VHT1nw6o%2BxTHrLenQebcps7ICRjTHGXcs9X9yM%3D&st=2019-11-08T06%3A32%3A52Z&se=2019-11-08T14%3A42%3A52Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://mtcsdevaml3870043637.blob.core.windows.net/azureml/ExperimentRun/dcid.2275167d-1ae7-4dd4-afbc-2dad13ac404f/logs/azureml/stdoutlogs.txt?sv=2018-11-09&sr=b&sig=%2FhWc1uMT9ilyAD7OM%2BQH9kw9vZDbVzrC3lqcDNrySHE%3D&st=2019-11-08T06%3A32%3A52Z&se=2019-11-08T14%3A42%3A52Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:00:09\"}, \"child_runs\": [{\"run_id\": \"9e7ea89d-8bb0-40d9-9fe3-98dfd42a177b\", \"name\": \"AML_Pipeline_ADB_Step\", \"status\": \"Failed\", \"start_time\": \"2019-11-08T06:42:23.355968Z\", \"created_time\": \"2019-11-08T06:42:23.355968Z\", \"end_time\": \"2019-11-08T06:42:26.035633Z\", \"duration\": \"0:00:02\", \"run_number\": 59, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-08T06:42:23.355968Z\", \"is_reused\": \"\"}, {\"run_id\": \"6f580a61-5d71-47ae-b095-fafb768c21c1\", \"name\": \"train_step\", \"status\": \"Finished\", \"start_time\": \"2019-11-08T06:42:22.728237Z\", \"created_time\": \"2019-11-08T06:42:22.728237Z\", \"end_time\": \"2019-11-08T06:42:22.789725Z\", \"duration\": \"0:00:00\", \"run_number\": 58, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-08T06:42:22.728237Z\", \"is_reused\": \"Yes\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2019-11-08 06:42:22Z] Completing processing run id 6f580a61-5d71-47ae-b095-fafb768c21c1.\\n[2019-11-08 06:42:23Z] Submitting run id 9e7ea89d-8bb0-40d9-9fe3-98dfd42a177b in experiment Hello_World2\\n[2019-11-08 06:42:27Z] Execution of experiment failed, update experiment status and cancel running nodes.\\n\", \"graph\": {\"datasource_nodes\": {\"dd440a74\": {\"node_id\": \"dd440a74\", \"name\": \"Data source for data path parameter intrem_data\"}, \"0b04a04e\": {\"node_id\": \"0b04a04e\", \"name\": \"test_data\"}}, \"module_nodes\": {\"35abaa6d\": {\"node_id\": \"35abaa6d\", \"name\": \"AML_Pipeline_ADB_Step\", \"status\": \"Failed\", \"_is_reused\": false, \"run_id\": \"9e7ea89d-8bb0-40d9-9fe3-98dfd42a177b\"}, \"62f04315\": {\"node_id\": \"62f04315\", \"name\": \"train_step\", \"status\": \"Finished\", \"_is_reused\": true, \"run_id\": \"6f580a61-5d71-47ae-b095-fafb768c21c1\"}}, \"edges\": [{\"source_node_id\": \"dd440a74\", \"source_node_name\": \"Data source for data path parameter intrem_data\", \"source_name\": \"data\", \"target_name\": \"output_61c636ef\", \"dst_node_id\": \"35abaa6d\", \"dst_node_name\": \"AML_Pipeline_ADB_Step\"}, {\"source_node_id\": \"62f04315\", \"source_node_name\": \"train_step\", \"source_name\": \"amlpipedata\", \"target_name\": \"output_61c636ef\", \"dst_node_id\": \"35abaa6d\", \"dst_node_name\": \"AML_Pipeline_ADB_Step\"}, {\"source_node_id\": \"0b04a04e\", \"source_node_name\": \"test_data\", \"source_name\": \"data\", \"target_name\": \"test_data\", \"dst_node_id\": \"62f04315\", \"dst_node_name\": \"train_step\"}], \"child_runs\": [{\"run_id\": \"9e7ea89d-8bb0-40d9-9fe3-98dfd42a177b\", \"name\": \"AML_Pipeline_ADB_Step\", \"status\": \"Failed\", \"start_time\": \"2019-11-08T06:42:23.355968Z\", \"created_time\": \"2019-11-08T06:42:23.355968Z\", \"end_time\": \"2019-11-08T06:42:26.035633Z\", \"duration\": \"0:00:02\", \"run_number\": 59, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-08T06:42:23.355968Z\", \"is_reused\": \"\"}, {\"run_id\": \"6f580a61-5d71-47ae-b095-fafb768c21c1\", \"name\": \"train_step\", \"status\": \"Finished\", \"start_time\": \"2019-11-08T06:42:22.728237Z\", \"created_time\": \"2019-11-08T06:42:22.728237Z\", \"end_time\": \"2019-11-08T06:42:22.789725Z\", \"duration\": \"0:00:00\", \"run_number\": 58, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-11-08T06:42:22.728237Z\", \"is_reused\": \"Yes\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.72\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8029c014b3924007891f45d3b4eb0870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"loading\": true}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Steps\n",
    "for step in pipeline_run.get_steps():\n",
    "    print(\"Outputs of step \" + step.name)\n",
    "    \n",
    "    # Get a dictionary of StepRunOutputs with the output name as the key \n",
    "    output_dict = step.get_outputs()\n",
    "    \n",
    "    for name, output in output_dict.items():\n",
    "        \n",
    "        output_reference = output.get_port_data_reference() # Get output port data reference\n",
    "        print(\"\\tname: \" + name)\n",
    "        print(\"\\tdatastore: \" + output_reference.datastore_name)\n",
    "        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"End of Notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sanpil"
   }
  ],
  "category": "tutorial",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "Custom"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML"
  ],
  "friendly_name": "Getting Started with Azure Machine Learning Pipelines",
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "order_index": 1,
  "tags": [
   "None"
  ],
  "task": "Getting Started notebook for ANML Pipelines"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
